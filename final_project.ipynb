{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Project - Group 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "1. [Explanatory Data Analysis](#1.-Explanatory-Data-Analysis)\n",
    "    - [Monthly Income](#Monthly-Income)\n",
    "    - [Revolving Utilization of Unsecured Lines](#Revolving-Utilization-Of-Unsecured-Lines)\n",
    "    - [Debt Ratio](#Debt-Ratio)\n",
    "    - [Age](#Age)\n",
    "    - [Number Of Open Credit Lines And Loans](#Number-Of-Open-Credit-Lines-And-Loans)\n",
    "    - [Number of Real Estate Loans and Lines](#Number-Of-Real-Estate-Loans-And-Lines)\n",
    "    - [Number of Dependents](#Number-Of-Dependents)\n",
    "    - [Number of N Days Past Due](#Number-Of-N-Days-Past-Due)\n",
    "2. [Data Cleaning](#Data-Cleaning)\n",
    "    - [Monthly Income - Missing Values](#Missing-Values)\n",
    "    - [Debt Ratio - Incorrect values](#Monthly-Income)\n",
    "3. [Feature Engineering](#Feature-Engineering)\n",
    "    - [Rare Case](#Missing-Values)\n",
    "4. [Preprocessing](#Preprocessing)\n",
    "\n",
    "5. [Model Selection](#Model-Selection)\n",
    "6. [Model Tuning](#Model-Tuning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import math \n",
    "import seaborn as sns\n",
    "from math import radians, sin, cos, sqrt, atan2 \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor, BaggingRegressor\n",
    "from sklearn.ensemble import StackingRegressor, VotingRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.datasets import make_regression\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('cs-test.csv')\n",
    "df_train.head()\n",
    "df_test.head()\n",
    "df_train.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "df_test.drop(columns=['Unnamed: 0', 'SeriousDlqin2yrs'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Explanatory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='SeriousDlqin2yrs', data=df_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unbalanced dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monthly Income "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot for 'RevolvingUtilizationOfUnsecuredLines' \n",
    "sns.boxplot(x='MonthlyIncome', data=df_train)\n",
    "plt.title('MonthlyIncome - BoxPlot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train['MonthlyIncome'].isna()].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train['MonthlyIncome'].isna()]['SeriousDlqin2yrs'].value_counts(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revolving Utilization Of Unsecured Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['RevolvingUtilizationOfUnsecuredLines'].describe().to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot for 'RevolvingUtilizationOfUnsecuredLines' \n",
    "sns.boxplot(x='RevolvingUtilizationOfUnsecuredLines', data=df_train)\n",
    "plt.title('RevolvingUtilizationOfUnsecuredLines - BoxPlot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot for 'RevolvingUtilizationOfUnsecuredLines' < 1\n",
    "import matplotlib.pyplot as plt\n",
    "sns.boxplot(x='RevolvingUtilizationOfUnsecuredLines', data=df_train[df_train['RevolvingUtilizationOfUnsecuredLines']<10])\n",
    "plt.title('RevolvingUtilizationOfUnsecuredLines < 10 - BoxPlot')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train['RevolvingUtilizationOfUnsecuredLines'] > 10].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximately 97.5% of values of this Variable are between 0 and 1 with a well defined right-skewed distribution. Generally, Credit Utilization is expected to be within the region (0 - 1). Altough, Borrowers can sometimes spend beyond credit limit. Values between 1 and 10 make up 2% of the dataset. Values beyond 10 are extremely big and they make up less than 0.5% of our data, these values would be dropped to prevent them from impacting our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debt Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['DebtRatio'].describe().to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot for DebtRatio, df_train\n",
    "sns.boxplot(df_train['DebtRatio'])\n",
    "plt.title('DebtRatio - BoxPlot')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many values for DebtRatio are between 0 and 2\n",
    "df_train['DebtRatio'].between(0, 2).sum()/len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train['DebtRatio'] > 2]['MonthlyIncome'].isnull().sum()/len(df_train[df_train['DebtRatio'] > 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train['DebtRatio'] > 2].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 79.4% of values in this variable are between 0 - 2\n",
    "- The remaining 20% have high values (Median of 1201). Outliers responsible for skewing the Variable\n",
    "- 90% of these values have a missing value for Monthly Income, this could mean that people forgot to insert the monthly income and the debt ratio was calculated having nothing as a denominator. \\\n",
    "We could try to predict Monthly Income, inserting some constraints based on the values of the debt ratio (e.g. if Debt Ratio = 0, then Monthly Income cannot be greater than 0). \\\n",
    "Then, we can adjust the Debt Ratios based on the new values of Monthly Income."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot for 'Age'\n",
    "sns.boxplot(x='age', data=df_train)\n",
    "plt.title('Age - BoxPlot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impossible to have a person under age. We should remove or substitute these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train['age']<18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['age'].replace(-1, 52, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replaced -1 value with the median of age equal to 52."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot for 'Age' in the test dataset\n",
    "sns.boxplot(x='age', data=df_test)\n",
    "plt.title('Age - BoxPlot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No underage individuals in the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumberOfOpenCreditLinesAndLoans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(18,6))\n",
    "sns.histplot(x = df_train['NumberOfOpenCreditLinesAndLoans'], binwidth=1, ax = axes[0])\n",
    "sns.histplot(x = df_test['NumberOfOpenCreditLinesAndLoans'], binwidth=1, ax = axes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This variable is right-skewed with no extreme values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the feature 'Number of Open Credit Lines and Loans' has integer values, it does not make sense that we keep decimal values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Real Estate Loans and Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(18,6))\n",
    "sns.histplot(x = df_train['NumberRealEstateLoansOrLines'], binwidth=1, ax = axes[0])\n",
    "sns.histplot(x = df_test['NumberRealEstateLoansOrLines'], binwidth=1, ax = axes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['NumberRealEstateLoansOrLines'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This variable is highly skewed to the right, Majority of the Borrowers have between 0 to 2 Mortgage loans. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Dependents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(18,6))\n",
    "sns.histplot(x = df_train['NumberOfDependents'], binwidth=1, ax = axes[0])\n",
    "sns.histplot(x = df_test['NumberOfDependents'], binwidth=1, ax = axes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This variable is right skewed. Majority of the Borrowers have between 0 - 3 Dependents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of N Days Past Due"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "due_30_59 = pd.DataFrame(df_train['NumberOfTime30-59DaysPastDueNotWorse'].value_counts()).rename(columns = {'NumberOfTime30-59DaysPastDueNotWorse':'30-59days'})\n",
    "due_60_89 =  pd.DataFrame(df_train['NumberOfTime60-89DaysPastDueNotWorse'].value_counts()).rename(columns = {'NumberOfTime60-89DaysPastDueNotWorse':'60-89days'})\n",
    "due_90 = pd.DataFrame(df_train['NumberOfTimes90DaysLate'].value_counts()).rename(columns = {'NumberOfTimes90DaysLate':'90days'})\n",
    "pd.concat([due_30_59, due_60_89, due_90], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_needed = ['NumberOfTime30-59DaysPastDueNotWorse', 'NumberOfTime60-89DaysPastDueNotWorse',\n",
    "                  'NumberOfTimes90DaysLate', 'SeriousDlqin2yrs']  \n",
    "data_filtered = df_train[columns_needed]\n",
    "\n",
    "mask = (data_filtered['NumberOfTime30-59DaysPastDueNotWorse'].isin([96, 98]) |\n",
    "        data_filtered['NumberOfTime60-89DaysPastDueNotWorse'].isin([96, 98]) |\n",
    "        data_filtered['NumberOfTimes90DaysLate'].isin([96, 98]))\n",
    "\n",
    "delinquency_data = data_filtered[mask]\n",
    "\n",
    "serious_delinquency_count = delinquency_data['SeriousDlqin2yrs'].sum()\n",
    "total_cases = delinquency_data.shape[0]\n",
    "\n",
    "serious_delinquency_count/total_cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These Features have similar distribution. There are two unique values (98 and 96). It is impossible for a borrower to exhibit delinquency 98 or 96 times in space of 2 years. It can also be observerd that these values share the same corresponding index, which might indicates Data Entry error. However, they can't be dropped due to high information they possess in identifying defaulting members. 54% of Borrowers in this category defaulted compared to 6% global default rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "corr_matrix = df_train.corr()\n",
    "plt.figure(figsize=(12, 10))\n",
    "ax = sns.heatmap(corr_matrix, fmt=\".2f\", cmap='Reds', square=True, linewidths=.5, cbar_kws={\"shrink\": .8})\n",
    "plt.title('Correlation Matrix')\n",
    "plt.xticks(fontstyle='italic')\n",
    "plt.yticks(fontstyle='italic')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of N Days Past Due - 96 and 98 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_df = df_train[df_train['NumberOfTimes90DaysLate']>90]\n",
    "len(rare_df[rare_df['DebtRatio']==0])/len(rare_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "53% of these data have 1s for the target feature, it means they are important to understand what determines the probability of facing serious deliquency. \\\n",
    "But we also can observe that 80% of these values have 0 as DebtRatio, so it's impossible for them to be past due."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rare_df[(rare_df['SeriousDlqin2yrs']==1)&(rare_df['DebtRatio']==0)])/len(rare_df[rare_df['SeriousDlqin2yrs']==1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the rare case, 72% of people having debtratio = 0 and seriousdel = 1. How can it be possible that a person with no debt has a deliquency?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(len(rare_df[rare_df['DebtRatio']>5])+len(rare_df[rare_df['DebtRatio']==0]))/len(rare_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_df[rare_df['SeriousDlqin2yrs']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of Open Credit Lines and Loans has very small mean, same thing for number of real estate loans or lines. \\\n",
    "Therefore we can consider them as errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_df[rare_df['SeriousDlqin2yrs']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_df[rare_df['MonthlyIncome'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_df[rare_df['MonthlyIncome'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "corr_matrix = df_train.corr()\n",
    "plt.figure(figsize=(12, 10))\n",
    "ax = sns.heatmap(corr_matrix, fmt=\".2f\", cmap='Reds', square=True, linewidths=.5, cbar_kws={\"shrink\": .8})\n",
    "plt.title('Correlation Matrix with Rare Case')\n",
    "plt.xticks(fontstyle='italic')\n",
    "plt.yticks(fontstyle='italic')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_filter = ['NumberOfTime30-59DaysPastDueNotWorse', 'NumberOfTimes90DaysLate', 'NumberOfTime60-89DaysPastDueNotWorse']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extreme values - RevolvingUtilizationOfUnsecuredLines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revolv = df_train[df_train['RevolvingUtilizationOfUnsecuredLines'] > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revolv_test = df_test[df_test['RevolvingUtilizationOfUnsecuredLines'] > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(revolv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(revolv[revolv['SeriousDlqin2yrs'] == 1])/len(revolv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[df_train['RevolvingUtilizationOfUnsecuredLines'] <= 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monthly Income - Missing Values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_no_missing_income = df_train[df_train['MonthlyIncome'].notna()]\n",
    "\n",
    "debt_ratio_condition = (data_no_missing_income['DebtRatio'] >= 0) & (data_no_missing_income['DebtRatio'] <= 2)\n",
    "percentage_in_range = (debt_ratio_condition.sum() / len(data_no_missing_income)) * 100\n",
    "\n",
    "print(f\"Percentage of DebtRatio values between 0 and 2: {percentage_in_range:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "number_imputer = SimpleImputer(strategy='median')\n",
    "df_train['NumberOfDependents'] = number_imputer.fit_transform(df_train[['NumberOfDependents']])\n",
    "\n",
    "missing_income_indexes = df_train[df_train['MonthlyIncome'].isna()].index\n",
    "\n",
    "features = ['RevolvingUtilizationOfUnsecuredLines', 'age', 'DebtRatio', \n",
    "            'NumberOfOpenCreditLinesAndLoans', 'NumberRealEstateLoansOrLines']\n",
    "data_with_income = df_train[df_train['MonthlyIncome'].notna()][features + ['MonthlyIncome']]\n",
    "\n",
    "X = data_with_income.drop(columns=['MonthlyIncome'])\n",
    "y = data_with_income['MonthlyIncome']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "'''\n",
    "models = {\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    'RandomForestRegressor': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'GradientBoostingRegressor': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "    'SupportVectorRegression': SVR()\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    results[name] = mse\n",
    "\n",
    "best_model_name = min(results, key=results.get)\n",
    "best_model = models[best_model_name]\n",
    "'''\n",
    "monthly_model = LinearRegression()\n",
    "monthly_model.fit(X_train, y_train)\n",
    "y_pred = monthly_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "X_missing = df_train.loc[missing_income_indexes][features]\n",
    "predicted_income = monthly_model.predict(X_missing)\n",
    "\n",
    "predicted_income = np.maximum(predicted_income, 0)\n",
    "\n",
    "df_train.loc[missing_income_indexes, 'MonthlyIncome'] = predicted_income\n",
    "\n",
    "df_train.loc[missing_income_indexes, 'DebtRatio'] /= df_train.loc[missing_income_indexes, 'MonthlyIncome']\n",
    "\n",
    "print(f\"Best model: {monthly_model} with MSE: {mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['NumberOfDependents'] = number_imputer.fit_transform(df_test[['NumberOfDependents']])\n",
    "\n",
    "# Predict MonthlyIncome on df_submission where it's missing\n",
    "missing_income_indexes_submission = df_test[df_test['MonthlyIncome'].isna()].index\n",
    "X_missing_submission = df_test.loc[missing_income_indexes_submission][features]\n",
    "\n",
    "predicted_income_submission = monthly_model.predict(X_missing_submission)\n",
    "predicted_income_submission = np.maximum(predicted_income_submission, 0)  # Ensure non-negative income predictions\n",
    "\n",
    "# Assign predicted MonthlyIncome to df_submission\n",
    "df_test.loc[missing_income_indexes_submission, 'MonthlyIncome'] = predicted_income_submission\n",
    "\n",
    "# Adjust DebtRatio in df_submission based on new MonthlyIncome\n",
    "df_test.loc[missing_income_indexes_submission, 'DebtRatio'] /= df_test.loc[missing_income_indexes_submission, 'MonthlyIncome']\n",
    "\n",
    "# Ensure no division by zero or near zero which could distort DebtRatio significantly\n",
    "df_test.loc[missing_income_indexes_submission, 'DebtRatio'] = df_test.loc[missing_income_indexes_submission, 'DebtRatio'].replace([np.inf, -np.inf], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Count of 'DebtRatio' between 0 and 2: {df_train['DebtRatio'].between(-1, 2).sum()}\")\n",
    "print(f\"Count of 'DebtRatio' beyond 2: {(df_train['DebtRatio'] > 2).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['RareCase'] = (df_train['NumberOfTimes90DaysLate'].isin([96, 98])).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_30_59 = df_train['NumberOfTime30-59DaysPastDueNotWorse'].median()\n",
    "median_60_89 = df_train['NumberOfTime60-89DaysPastDueNotWorse'].median()\n",
    "median_90 = df_train['NumberOfTimes90DaysLate'].median()\n",
    "\n",
    "df_train.loc[df_train['NumberOfTime30-59DaysPastDueNotWorse'].isin([96, 98]), 'NumberOfTime30-59DaysPastDueNotWorse'] = median_30_59\n",
    "df_train.loc[df_train['NumberOfTime60-89DaysPastDueNotWorse'].isin([96, 98]), 'NumberOfTime60-89DaysPastDueNotWorse'] = median_60_89\n",
    "df_train.loc[df_train['NumberOfTimes90DaysLate'].isin([96, 98]), 'NumberOfTimes90DaysLate'] = median_90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train['NumberOfTimes90DaysLate']>90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.replace([np.inf, -np.inf], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['RareCase'] = (df_test['NumberOfTimes90DaysLate'].isin([96, 98])).astype(int)\n",
    "median_30_59 = df_test['NumberOfTime30-59DaysPastDueNotWorse'].median()\n",
    "median_60_89 = df_test['NumberOfTime60-89DaysPastDueNotWorse'].median()\n",
    "median_90 = df_test['NumberOfTimes90DaysLate'].median()\n",
    "\n",
    "df_test.loc[df_test['NumberOfTime30-59DaysPastDueNotWorse'].isin([96, 98]), 'NumberOfTime30-59DaysPastDueNotWorse'] = median_30_59\n",
    "df_test.loc[df_test['NumberOfTime60-89DaysPastDueNotWorse'].isin([96, 98]), 'NumberOfTime60-89DaysPastDueNotWorse'] = median_60_89\n",
    "df_test.loc[df_test['NumberOfTimes90DaysLate'].isin([96, 98]), 'NumberOfTimes90DaysLate'] = median_90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def preprocess_data(train_df, test_df,  target, \n",
    "                    apply_undersampling=True, \n",
    "                    apply_scaling=True, \n",
    "                    apply_imputation=True, \n",
    "                    apply_pca=True,\n",
    "                    seed = 42,\n",
    "                    under_sampler = RandomUnderSampler(),\n",
    "                    scaler = StandardScaler(),\n",
    "                    imputer = SimpleImputer(strategy='mean'),\n",
    "                    pca = PCA(n_components=0.95)):\n",
    "    \n",
    "\n",
    "    under_sampler.set_params(random_state=seed)\n",
    "\n",
    "    # Split data into features and target\n",
    "    X = train_df.drop(columns=[target])\n",
    "    y = train_df[target]\n",
    "\n",
    "    cols = X.columns\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    if apply_undersampling:\n",
    "        X_train, y_train = under_sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "    if apply_scaling:\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        test_df = scaler.transform(test_df)\n",
    "\n",
    "    if apply_imputation:\n",
    "        X_train = imputer.fit_transform(X_train)\n",
    "        X_test = imputer.transform(X_test)\n",
    "        test_df = imputer.transform(test_df)\n",
    "\n",
    "    if apply_pca:\n",
    "        X_train = pca.fit_transform(X_train)\n",
    "        X_test = pca.transform(X_test)\n",
    "        cols = [f'PCA_{i}' for i in range(X_train.shape[1])]\n",
    "        test_df = pca.transform(test_df)\n",
    "\n",
    "    test_df = pd.DataFrame(test_df, columns=cols)\n",
    "\n",
    "    X_train_df = pd.DataFrame(X_train, columns=cols)\n",
    "    y_train_df = y_train\n",
    "\n",
    "    X_test_df = pd.DataFrame(X_test, columns=cols)\n",
    "    y_test_df = y_test\n",
    "\n",
    "    return X_train_df, X_test_df, y_train_df, y_test_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_roc_curve_and_cm(model, X_test, y_test):\n",
    "    # Compute probabilities\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Compute AUC\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    print(\"AUC: \", auc)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "\n",
    "    # Compute ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "    # Create subplots\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(7, 15))\n",
    "\n",
    "    # Plot ROC curve\n",
    "    ax[0].plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % auc)\n",
    "    ax[0].plot([0, 1], [0, 1], 'k--')\n",
    "    ax[0].set_xlim([0.0, 1.0])\n",
    "    ax[0].set_ylim([0.0, 1.05])\n",
    "    ax[0].set_xlabel('False Positive Rate')\n",
    "    ax[0].set_ylabel('True Positive Rate')\n",
    "    ax[0].set_title('Receiver Operating Characteristic')\n",
    "    ax[0].legend(loc=\"lower right\")\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax[1])\n",
    "    ax[1].set_title('Confusion Matrix')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df, X_test_df, y_train_df, y_test_df, test_for_submission = preprocess_data(df_train, df_test, 'SeriousDlqin2yrs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic with weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_no_undersampling, X_test_no_undersampling, y_train_no_undersampling, y_test_no_undersampling = preprocess_data(df_train, df_test, 'SeriousDlqin2yrs', apply_undersampling=False, apply_pca=False)\n",
    "weight = y_train_no_undersampling.value_counts(normalize=True)[0] / y_train_no_undersampling.value_counts(normalize=True)[1]\n",
    "\n",
    "lg_w_weight = LogisticRegression(class_weight={0: 1, 1: weight})\n",
    "\n",
    "lg_w_weight.fit(X_train_no_undersampling, y_train_no_undersampling)\n",
    "\n",
    "y_pred = lg_w_weight.predict(X_test_no_undersampling)\n",
    "\n",
    "accuracy = accuracy_score(y_test_no_undersampling, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "report = classification_report(y_test_no_undersampling, y_pred)\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve_and_cm(lg_w_weight, X_test_no_undersampling, y_test_no_undersampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = lg_w_weight.coef_[0]\n",
    "\n",
    "indices = np.argsort(np.abs(importance))[::-1]\n",
    "\n",
    "names = [X_train_no_undersampling.columns[i] for i in indices]\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.bar(range(X_train_no_undersampling.shape[1]), importance[indices])\n",
    "plt.xticks(range(X_train_no_undersampling.shape[1]), names, rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"XGBClassifier\": XGBClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"XGBoost\": XGBClassifier(),\n",
    "    \"LogisticRegression\": LogisticRegression(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"Ada Boosting\": AdaBoostClassifier(),\n",
    "    \"Support Vector Machine\": SVC(probability=True),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    model.fit(X_train_df, y_train_df)\n",
    "    plot_roc_curve_and_cm(model, X_test_df, y_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_all_roc_curves(models, X_test, y_test):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Plot each model's ROC curve\n",
    "    for name, model in models.items():\n",
    "        # Predict probabilities for the positive class\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # Calculate ROC AUC\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        \n",
    "        # Calculate ROC curve points\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "        \n",
    "        # Plot the ROC curve\n",
    "        plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')\n",
    "    \n",
    "    # Plot the no skill line\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    \n",
    "    # Add labels and legend\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curves Comparison')\n",
    "    plt.legend(loc='lower right')\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"Ada Boosting\": AdaBoostClassifier(),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "}\n",
    "\n",
    "# Assuming X_train_df, y_train_df, X_test_df, y_test_df are already defined and available\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    model.fit(X_train_df, y_train_df)\n",
    "\n",
    "# After training, pass the models and test data to plot the ROC curves\n",
    "plot_all_roc_curves(models, X_test_df, y_test_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the most important features for the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = GradientBoostingClassifier()\n",
    "\n",
    "best_model.fit(X_train_df, y_train_df)\n",
    "\n",
    "plot_roc_curve_and_cm(best_model, X_test_df, y_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Example setup\n",
    "# X_train, y_train are your features and target variable respectively\n",
    "# clf is your trained Gradient Boosting classifier\n",
    "clf = GradientBoostingClassifier()\n",
    "clf.fit(X_train_df, y_train_df)\n",
    "\n",
    "# Extract feature importances\n",
    "feature_importances = clf.feature_importances_\n",
    "\n",
    "# Creating a DataFrame to view and sort importances\n",
    "features = X_train_df.columns\n",
    "importances_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})\n",
    "importances_df = importances_df.sort_values(by='Importance', ascending=True)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(importances_df['Feature'], importances_df['Importance'], color='darkred')\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance from Gradient Boosting Classifier')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with highest accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianNB()\n",
    "\n",
    "model.fit(X_train_df, y_train_df)\n",
    "\n",
    "plot_roc_curve_and_cm(best_model, X_test_df, y_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prob_features = model.theta_  # You could also use sigma_ or other statistics\n",
    "\n",
    "# Plotting these \"importances\"\n",
    "# Note: We're using the mean here for demonstration; this isn't a typical \"importance\"\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "indices = np.argsort(log_prob_features[0])  # Sort by the first class, for example\n",
    "plt.title('Feature \"Importances\" in Gaussian Naive Bayes')\n",
    "plt.barh(range(len(indices)), log_prob_features[0][indices], color='darkred', align='center')\n",
    "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "plt.xlabel('Feature Mean Value (First Class)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "def optimize_model(model, param_distributions, X_train, y_train, n_iter=100, cv=5, random_state=42):\n",
    "    # Set up the random search with 5-fold cross validation\n",
    "    random_search = RandomizedSearchCV(estimator=model,\n",
    "                                       param_distributions=param_distributions,\n",
    "                                       n_iter=n_iter,\n",
    "                                       cv=cv,\n",
    "                                       random_state=random_state,\n",
    "                                       verbose = 2,\n",
    "                                       n_jobs=-1)\n",
    "\n",
    "    # Fit the random search model\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    return random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distributions = {\n",
    "    'n_estimators': [int(x) for x in np.linspace(start=200, stop=1000, num=10)],\n",
    "    'max_features': ['sqrt'],\n",
    "    'max_depth': [int(x) for x in np.linspace(5, 25, num=6)] + [None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# optimized_rf = optimize_model(RandomForestClassifier(), \n",
    "                                 # param_distributions, \n",
    "                                 # X_train_df, \n",
    "                                 # y_train_df, \n",
    "                                 # n_iter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimized_rf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_roc_curve_and_cm(optimized_rf, X_test_df, y_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xgboost optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "param_distributions = {\n",
    "    'n_estimators': [int(x) for x in np.linspace(start=100, stop=1000, num=10)],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "    'max_depth': [int(x) for x in np.linspace(3, 10, num=8)],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'gamma': [0, 0.1, 0.2, 0.3, 0.4],\n",
    "    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'objective': ['binary:logistic']\n",
    "}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "optimized_xgb = optimize_model(XGBClassifier(), \n",
    "                                 param_distributions, \n",
    "                                 X_train_df, \n",
    "                                 y_train_df, \n",
    "                                 n_iter=20)\n",
    "            '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_roc_curve_and_cm(optimized_xgb, X_test_df, y_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization for gradient boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distributions = {\n",
    "    'n_estimators': [50, 100],  # Reduced maximum number, fewer steps\n",
    "    'learning_rate': [0.1, 0.2, 0.3],  # Higher and fewer options\n",
    "    'max_depth': [3, 5],  # Reduced complexity\n",
    "    'min_samples_split': [5, 10],  # Reduced variation\n",
    "    'min_samples_leaf': [2, 4]  # Fewer options, more generalization\n",
    "}\n",
    "\n",
    "# Assuming X_train_df and y_train_df are defined\n",
    "optimized_gb = optimize_model(GradientBoostingClassifier(), \n",
    "                              param_distributions,\n",
    "                              X_train_df, \n",
    "                              y_train_df, \n",
    "                              n_iter=10)  # Reduced the number of iterations\n",
    "\n",
    "print(\"Optimized model:\", optimized_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve_and_cm(optimized_gb, X_test_df, y_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model with 'log_loss' which supports probability predictions\n",
    "best_model = GradientBoostingClassifier(max_depth=5, min_samples_leaf=4, min_samples_split=5,\n",
    "                           n_estimators=50, loss='log_loss')\n",
    "\n",
    "# Train the model with your training data\n",
    "best_model.fit(X_train_df, y_train_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = test_for_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "features = ['RevolvingUtilizationOfUnsecuredLines', 'age', 'DebtRatio', \n",
    "            'NumberOfOpenCreditLinesAndLoans', 'NumberRealEstateLoansOrLines']\n",
    "\n",
    "df_test['NumberOfDependents'] = number_imputer.fit_transform(df_test[['NumberOfDependents']])\n",
    "\n",
    "# Predict MonthlyIncome on df_submission where it's missing\n",
    "missing_income_indexes_submission = df_test[df_test['MonthlyIncome'].isna()].index\n",
    "X_missing_submission = df_test.loc[missing_income_indexes_submission][features]\n",
    "\n",
    "predicted_income_submission = monthly_model.predict(X_missing_submission)\n",
    "predicted_income_submission = np.maximum(predicted_income_submission, 0)  # Ensure non-negative income predictions\n",
    "\n",
    "# Assign predicted MonthlyIncome to df_submission\n",
    "df_test.loc[missing_income_indexes_submission, 'MonthlyIncome'] = predicted_income_submission\n",
    "\n",
    "# Adjust DebtRatio in df_submission based on new MonthlyIncome\n",
    "df_test.loc[missing_income_indexes_submission, 'DebtRatio'] /= df_test.loc[missing_income_indexes_submission, 'MonthlyIncome']\n",
    "\n",
    "# Ensure no division by zero or near zero which could distort DebtRatio significantly\n",
    "df_test.loc[missing_income_indexes_submission, 'DebtRatio'] = df_test.loc[missing_income_indexes_submission, 'DebtRatio'].replace([np.inf, -np.inf], 0)\n",
    "\n",
    "# Optionally, save the updated df_submission to a new CSV file\n",
    "# df_submission.to_csv('path_to_save_preprocessed_submission.csv', index=False)\n",
    "\n",
    "print(\"Preprocessing and prediction completed for submission dataset.\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit(model, test_df, filename='submission3.csv'):\n",
    "    \"\"\"\n",
    "    Generate a CSV submission file containing the probabilities of the positive class.\n",
    "    \n",
    "    :param model: Trained Gradient Boosting model.\n",
    "    :param test_df: DataFrame containing the test features.\n",
    "    :param filename: The name of the file to save the predictions to.\n",
    "    \"\"\"\n",
    "    # Ensure the input DataFrame for predictions is in the correct format\n",
    "    test_array = np.array(test_df)  # Convert DataFrame to numpy array if needed for the model\n",
    "\n",
    "    # Predict the probabilities for the positive class\n",
    "    probabilities = model.predict_proba(test_array)[:, 1]  # Index 1 for class '1'\n",
    "\n",
    "    test_df.index = range(1, len(test_df) + 1)\n",
    "\n",
    "    # Create a new DataFrame for submission using the adjusted DataFrame index as ID\n",
    "    submission_df = pd.DataFrame({\n",
    "        'id': test_df.index,  # Use the adjusted index as the ID\n",
    "        'probability': probabilities\n",
    "    })\n",
    "\n",
    "\n",
    "    # Save the DataFrame to a CSV file for submission\n",
    "    submission_df.to_csv(filename, index=False)\n",
    "    print(\"Submission file created successfully and saved to:\", filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit(best_model, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('submission3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
